{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Dmitry\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Dmitry\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-mnist\"\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "sagemaker_role = iam.get_role(RoleName='SagemakerRole')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "MNIST.mirrors = [\n",
    "    f\"https://sagemaker-example-files-prod-{region}.s3.amazonaws.com/datasets/image/MNIST/\"\n",
    "]\n",
    "\n",
    "MNIST(\n",
    "    \"data\",\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-514157727917/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "\n",
      "#import sagemaker_containers\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.optim as optim\n",
      "import torch.utils.data\n",
      "import torch.utils.data.distributed\n",
      "from torchvision import datasets, transforms\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
      "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
      "        self.conv2_drop = nn.Dropout2d()\n",
      "        self.fc1 = nn.Linear(320, 50)\n",
      "        self.fc2 = nn.Linear(50, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
      "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
      "        x = x.view(-1, 320)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.dropout(x, training=self.training)\n",
      "        x = self.fc2(x)\n",
      "        return F.log_softmax(x, dim=1)\n",
      "\n",
      "\n",
      "def _get_train_data_loader(batch_size, training_dir, is_distributed, **kwargs):\n",
      "    logger.info(\"Get train data loader\")\n",
      "    dataset = datasets.MNIST(\n",
      "        training_dir,\n",
      "        train=True,\n",
      "        transform=transforms.Compose(\n",
      "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
      "        ),\n",
      "    )\n",
      "    train_sampler = (\n",
      "        torch.utils.data.distributed.DistributedSampler(dataset) if is_distributed else None\n",
      "    )\n",
      "    return torch.utils.data.DataLoader(\n",
      "        dataset,\n",
      "        batch_size=batch_size,\n",
      "        shuffle=train_sampler is None,\n",
      "        sampler=train_sampler,\n",
      "        **kwargs\n",
      "    )\n",
      "\n",
      "\n",
      "def _get_test_data_loader(test_batch_size, training_dir, **kwargs):\n",
      "    logger.info(\"Get test data loader\")\n",
      "    return torch.utils.data.DataLoader(\n",
      "        datasets.MNIST(\n",
      "            training_dir,\n",
      "            train=False,\n",
      "            transform=transforms.Compose(\n",
      "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
      "            ),\n",
      "        ),\n",
      "        batch_size=test_batch_size,\n",
      "        shuffle=True,\n",
      "        **kwargs\n",
      "    )\n",
      "\n",
      "\n",
      "def _average_gradients(model):\n",
      "    # Gradient averaging.\n",
      "    size = float(dist.get_world_size())\n",
      "    for param in model.parameters():\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
      "        param.grad.data /= size\n",
      "\n",
      "\n",
      "def train(args):\n",
      "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
      "    logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
      "    use_cuda = args.num_gpus > 0\n",
      "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
      "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
      "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
      "\n",
      "    if is_distributed:\n",
      "        # Initialize the distributed environment.\n",
      "        world_size = len(args.hosts)\n",
      "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
      "        host_rank = args.hosts.index(args.current_host)\n",
      "        os.environ[\"RANK\"] = str(host_rank)\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
      "        logger.info(\n",
      "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
      "                args.backend, dist.get_world_size()\n",
      "            )\n",
      "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), args.num_gpus)\n",
      "        )\n",
      "\n",
      "    # set the seed for generating random numbers\n",
      "    torch.manual_seed(args.seed)\n",
      "    if use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\n",
      "\n",
      "    logger.debug(\n",
      "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
      "            len(train_loader.sampler),\n",
      "            len(train_loader.dataset),\n",
      "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    logger.debug(\n",
      "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
      "            len(test_loader.sampler),\n",
      "            len(test_loader.dataset),\n",
      "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    model = Net().to(device)\n",
      "    if is_distributed and use_cuda:\n",
      "        # multi-machine multi-gpu case\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
      "    else:\n",
      "        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
      "        model = torch.nn.DataParallel(model)\n",
      "\n",
      "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "\n",
      "    for epoch in range(1, args.epochs + 1):\n",
      "        model.train()\n",
      "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            optimizer.zero_grad()\n",
      "            output = model(data)\n",
      "            loss = F.nll_loss(output, target)\n",
      "            loss.backward()\n",
      "            if is_distributed and not use_cuda:\n",
      "                # average gradients manually for multi-machine cpu case only\n",
      "                _average_gradients(model)\n",
      "            optimizer.step()\n",
      "            if batch_idx % args.log_interval == 0:\n",
      "                logger.info(\n",
      "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
      "                        epoch,\n",
      "                        batch_idx * len(data),\n",
      "                        len(train_loader.sampler),\n",
      "                        100.0 * batch_idx / len(train_loader),\n",
      "                        loss.item(),\n",
      "                    )\n",
      "                )\n",
      "        test(model, test_loader, device)\n",
      "    save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "def test(model, test_loader, device):\n",
      "    model.eval()\n",
      "    test_loss = 0\n",
      "    correct = 0\n",
      "    with torch.no_grad():\n",
      "        for data, target in test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
      "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= len(test_loader.dataset)\n",
      "    logger.info(\n",
      "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
      "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "def model_fn(model_dir):\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "    model = torch.nn.DataParallel(Net())\n",
      "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    return model.to(device)\n",
      "\n",
      "\n",
      "def save_model(model, model_dir):\n",
      "    logger.info(\"Saving the model.\")\n",
      "    path = os.path.join(model_dir, \"model.pth\")\n",
      "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    # Data and model checkpoints directories\n",
      "    parser.add_argument(\n",
      "        \"--batch-size\",\n",
      "        type=int,\n",
      "        default=64,\n",
      "        metavar=\"N\",\n",
      "        help=\"input batch size for training (default: 64)\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--test-batch-size\",\n",
      "        type=int,\n",
      "        default=1000,\n",
      "        metavar=\"N\",\n",
      "        help=\"input batch size for testing (default: 1000)\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--epochs\",\n",
      "        type=int,\n",
      "        default=10,\n",
      "        metavar=\"N\",\n",
      "        help=\"number of epochs to train (default: 10)\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\"\n",
      "    )\n",
      "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\n",
      "    parser.add_argument(\n",
      "        \"--log-interval\",\n",
      "        type=int,\n",
      "        default=100,\n",
      "        metavar=\"N\",\n",
      "        help=\"how many batches to wait before logging training status\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--backend\",\n",
      "        type=str,\n",
      "        default=None,\n",
      "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
      "    )\n",
      "\n",
      "    # Container environment\n",
      "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
      "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
      "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
      "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
      "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
      "\n",
      "    train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "!pygmentize mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Dmitry\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"mnist.py\",\n",
    "    role=sagemaker_role,\n",
    "    py_version=\"py38\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    hyperparameters={\"epochs\": 1, \"backend\": \"gloo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-20-12-53-41-160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-20 12:53:45 Starting - Starting the training job...\n",
      "2023-09-20 12:54:00 Starting - Preparing the instances for training......\n",
      "2023-09-20 12:55:03 Downloading - Downloading input data...\n",
      "2023-09-20 12:55:33 Training - Downloading the training image...\n",
      "2023-09-20 12:56:19 Training - Training image download completed. Training in progress.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-09-20 08:56:28,937 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-09-20 08:56:28,938 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:28,940 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:28,950 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-09-20 08:56:28,952 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-09-20 08:56:29,168 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:29,171 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:29,183 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:29,185 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:29,198 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:29,200 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:29,210 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-09-20-12-53-41-160\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-514157727917/pytorch-training-2023-09-20-12-53-41-160/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"backend\":\"gloo\",\"epochs\":1}\n",
      "SM_USER_ENTRY_POINT=mnist.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.c5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=mnist\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-514157727917/pytorch-training-2023-09-20-12-53-41-160/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.c5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-09-20-12-53-41-160\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-514157727917/pytorch-training-2023-09-20-12-53-41-160/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\n",
      "SM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_BACKEND=gloo\n",
      "SM_HP_EPOCHS=1\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.8 mnist.py --backend gloo --epochs 1\n",
      "2023-09-20 08:56:29,652 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Distributed training - True\n",
      "Number of gpus available - 0\n",
      "Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\n",
      "Get train data loader\n",
      "Traceback (most recent call last):\n",
      "  File \"mnist.py\", line 257, in <module>\n",
      "train(parser.parse_args())\n",
      "  File \"mnist.py\", line 114, in train\n",
      "train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
      "  File \"mnist.py\", line 44, in _get_train_data_loader\n",
      "dataset = datasets.MNIST(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 102, in __init__\n",
      "raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
      "RuntimeError: Dataset not found. You can use download=True to download it\n",
      "2023-09-20 08:56:32,067 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-09-20 08:56:32,067 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "2023-09-20 08:56:32,068 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2023-09-20 08:56:32,068 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
      " RuntimeError: Dataset not found. You can use download=True to download it\"\n",
      "Command \"/opt/conda/bin/python3.8 mnist.py --backend gloo --epochs 1\"\n",
      "2023-09-20 08:56:32,068 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-09-20 08:56:30,646 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-09-20 08:56:30,647 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:30,649 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:30,658 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-09-20 08:56:30,660 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-09-20 08:56:30,856 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:30,858 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:30,870 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:30,872 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:30,885 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-09-20 08:56:30,887 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-20 08:56:30,897 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-09-20-12-53-41-160\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-514157727917/pytorch-training-2023-09-20-12-53-41-160/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"backend\":\"gloo\",\"epochs\":1}\n",
      "SM_USER_ENTRY_POINT=mnist.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.c5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=mnist\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-514157727917/pytorch-training-2023-09-20-12-53-41-160/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.c5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-09-20-12-53-41-160\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-514157727917/pytorch-training-2023-09-20-12-53-41-160/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\n",
      "SM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_BACKEND=gloo\n",
      "SM_HP_EPOCHS=1\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.8 mnist.py --backend gloo --epochs 1\n",
      "2023-09-20 08:56:31,355 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Distributed training - True\n",
      "Number of gpus available - 0\n",
      "Initialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\n",
      "Get train data loader\n",
      "Traceback (most recent call last):\n",
      "  File \"mnist.py\", line 257, in <module>\n",
      "train(parser.parse_args())\n",
      "  File \"mnist.py\", line 114, in train\n",
      "train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
      "  File \"mnist.py\", line 44, in _get_train_data_loader\n",
      "dataset = datasets.MNIST(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 102, in __init__\n",
      "raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
      "RuntimeError: Dataset not found. You can use download=True to download it\n",
      "2023-09-20 08:56:32,083 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-09-20 08:56:32,083 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "2023-09-20 08:56:32,084 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2023-09-20 08:56:32,084 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
      " RuntimeError: Dataset not found. You can use download=True to download it\"\n",
      "Command \"/opt/conda/bin/python3.8 mnist.py --backend gloo --epochs 1\"\n",
      "2023-09-20 08:56:32,084 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "\n",
      "2023-09-20 12:57:23 Uploading - Uploading generated training model\n",
      "2023-09-20 12:57:23 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2023-09-20-12-53-41-160: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n RuntimeError: Dataset not found. You can use download=True to download it\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --backend gloo --epochs 1\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32ma:\\algoritm\\DishRecognizerSP\\sagemaker.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/a%3A/algoritm/DishRecognizerSP/sagemaker.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m estimator\u001b[39m.\u001b[39;49mfit({\u001b[39m\"\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m\"\u001b[39;49m: inputs})\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[0;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\estimator.py:1310\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[0;32m   1309\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m-> 1310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\estimator.py:2580\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 2580\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[0;32m   2581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:4849\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[0;32m   4828\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   4829\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[0;32m   4830\u001b[0m \n\u001b[0;32m   4831\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4847\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[0;32m   4848\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4849\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_session, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:6760\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[1;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[0;32m   6757\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[0;32m   6759\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m-> 6760\u001b[0m     _check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   6761\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[0;32m   6762\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:6813\u001b[0m, in \u001b[0;36m_check_job_status\u001b[1;34m(job, desc, status_key_name)\u001b[0m\n\u001b[0;32m   6807\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[0;32m   6808\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[0;32m   6809\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[0;32m   6810\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   6811\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[0;32m   6812\u001b[0m     )\n\u001b[1;32m-> 6813\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[0;32m   6814\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[0;32m   6815\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   6816\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[0;32m   6817\u001b[0m )\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2023-09-20-12-53-41-160: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n RuntimeError: Dataset not found. You can use download=True to download it\"\nCommand \"/opt/conda/bin/python3.8 mnist.py --backend gloo --epochs 1\", exit code: 1"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
